{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hywo_4jiGSDm"
      },
      "outputs": [],
      "source": [
        "# Social Media Monitoring - NLP + Machine Learning\n",
        "# Executable in Google Colab.\n",
        "# Updated version: Allows uploading dataset directly in Colab.\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Install required packages (run in Colab)\n",
        "# ------------------------------\n",
        "!pip install --quiet scikit-learn pandas matplotlib seaborn nltk wordcloud imbalanced-learn\n",
        "\n",
        "# Optional (transformer model) - uncomment if you have GPU and want a transformer baseline\n",
        "# !pip install --quiet transformers torch sentencepiece\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Imports\n",
        "# ------------------------------\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP helpers\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# For dealing with class imbalance (if present)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ------------------------------\n",
        "# 3. NLTK downloads (first run)\n",
        "# ------------------------------\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Upload dataset directly in Colab\n",
        "# ------------------------------\n",
        "from google.colab import files\n",
        "print('Please upload your dataset CSV file...')\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "print(f\"\\nUploaded file: {filename}\")\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "print('\\nDataset shape:', df.shape)\n",
        "print('\\nPreview:')\n",
        "print(df.head())\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Expected columns and quick fixes\n",
        "# ------------------------------\n",
        "possible_text_cols = ['text','tweet','content','message','post']\n",
        "possible_label_cols = ['label','sentiment','target','class']\n",
        "\n",
        "text_col = None\n",
        "label_col = None\n",
        "for c in df.columns:\n",
        "    if c.lower() in possible_text_cols:\n",
        "        text_col = c\n",
        "        break\n",
        "for c in df.columns:\n",
        "    if c.lower() in possible_label_cols:\n",
        "        label_col = c\n",
        "        break\n",
        "\n",
        "if text_col is None or label_col is None:\n",
        "    print('\\nCould not auto-detect text/label columns. Detected columns:')\n",
        "    print(list(df.columns))\n",
        "    raise SystemExit('Please rename your text column to one of: ' + str(possible_text_cols) + '\\nand label column to one of: ' + str(possible_label_cols))\n",
        "\n",
        "df = df[[text_col, label_col]].rename(columns={text_col: 'text', label_col: 'label'})\n",
        "df = df.dropna(subset=['text','label']).reset_index(drop=True)\n",
        "\n",
        "print('\\nLabel distribution:')\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Basic EDA\n",
        "# ------------------------------\n",
        "print('\\nNumber of examples:', len(df))\n",
        "print('\\nSample texts:')\n",
        "print(df['text'].sample(5).values)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.title('Label distribution')\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 7. Preprocessing utilities\n",
        "# ------------------------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = re.sub(r'#(\\w+)', lambda m: m.group(1), text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print('\\nCleaning texts (this may take a minute)...')\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print('Done.')\n",
        "\n",
        "# ------------------------------\n",
        "# 8. Train / Test split\n",
        "# ------------------------------\n",
        "# ------------------------------\n",
        "# 8. Train / Test split (Fixed)\n",
        "# ------------------------------\n",
        "label_counts = y.value_counts()\n",
        "\n",
        "if (label_counts < 2).any():\n",
        "    print(\"⚠️ Some classes have fewer than 2 samples. Using non-stratified split.\")\n",
        "    stratify_option = None\n",
        "else:\n",
        "    stratify_option = y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=stratify_option, random_state=42\n",
        ")\n",
        "\n",
        "print('\\nTrain size:', len(X_train), 'Test size:', len(X_test))\n",
        "print('Label distribution in train set:')\n",
        "print(y_train.value_counts())\n",
        "print('\\\\nLabel distribution in test set:')\n",
        "print(y_test.value_counts())\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 9. Train and Evaluate Models\n",
        "# ------------------------------\n",
        "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "\n",
        "def train_and_evaluate(model_name, model):\n",
        "    pipe = Pipeline([\n",
        "        ('tfidf', tfidf),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_test)\n",
        "    print(f'\\nModel: {model_name}')\n",
        "    print('Accuracy:', accuracy_score(y_test, preds))\n",
        "    print('Classification Report:\\n', classification_report(y_test, preds))\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "    return pipe\n",
        "\n",
        "# Logistic Regression\n",
        "lr_model = train_and_evaluate('Logistic Regression', LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Linear SVM\n",
        "svm_model = train_and_evaluate('Linear SVM', LinearSVC(max_iter=10000))\n",
        "\n",
        "# Random Forest\n",
        "rf_model = train_and_evaluate('Random Forest', RandomForestClassifier(n_estimators=200))"
      ]
    }
  ]
}